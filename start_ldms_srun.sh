#! /bin/bash
# Author: Benjamin Allan
# Sandia National Laboratories
# 01/19/2022
#
# ldmsd startup script called from sbatch input scripts
# setup environment variables/files for ldmsd and run for user
#
# arguments: job_id job_dir ldms_slurmenv plugin_config
#
# job_id: slurm job id
# job_dir: where to dump all the ldms-related files.
# ldms_slurmenv: environment file for ldmsd -- generated by default; may be
#                overridden if another file following the same variable
#                conventions is wanted.
# plugin_config: ldms script commands file to start samplers/(aggs)/storage.
#                generated by default and may be overridden or edited
#                once generated.
#
# Requires:
# a) ldmsd installed from source
# b) the arguments above
#
# Affected by environment variables:
# LDMSD_LOG_LEVEL (unset or one of ERROR,WARNING,INFO,DEBUG,CRITICAL)
# LDMS_PREFIX (set this in your shell environment, or Ben's at Sandia
#   is assumed {which usually fails elsewhere}).
# LDMSAGGD_CONNECTION_RETRY_INTERVAL (default 1000000 (microsec))
# LDMSD_INTERVAL_DEFAULT (default 1000000 (microsec) sampler interval)
# ldms_prefix (see below)
# ldms_startclean if set 1, wipes out previously existing
#   sampler and env var configuration files named in argv.

# This line must be adjusted to point at your ldms installation
# if you don't set LDMS_PREFIX in your shell.
#set -x
#set -e
if test -z "$LDMS_PREFIX"; then
	echo "Missing LDMS_PREFIX!!!"
	exit
else
	ldms_prefix=$LDMS_PREFIX
fi
export ldms_prefix

# HOSTNAME is propagated by slurm, so we cannot use the shell variable
MYhostname=`hostname`
export MYhostname

####################################################################
# Creates a file in HOME for use with all jobs under ldms
# Once created, it can be adjusted manually and will not be
# overwritten.
# Multiple customized files can be managed for various jobs.
function ldms_default_env {
	# do not edit this function
	if test -f $1; then
		echo "Using existing $1 for ldmsd environment"
		return
	fi
	echo "Warning: generating default ldms environment file $1."
	cat << EOF > $1
#!/bin/false
# this script runs within the start_ldms.sh and requires preset variables:
# ldms_prefix, jdir
# LDMS installation data
. \$ldms_prefix/lib/ovis-lib-configvars.sh
. \$ldms_prefix/lib/ovis-ldms-configvars.sh
export PATH=\${ldms_prefix}/sbin:\${ldms_prefix}/bin:\$PATH
export LD_LIBRARY_PATH=\${ovis_ldms_libdir}:\${ovis_lib_libdir}:\${ovis_ldms_pkglibdir}:\${ovis_lib_pkglibdir}:\$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=\$LIBEVENT:\$LD_LIBRARY_PATH
export ZAP_LIBPATH=\${ovis_lib_pkglibdir}
export LDMSD_PLUGIN_LIBPATH=\${ovis_ldms_pkglibdir}
# output controls
export TESTDIR=\$jdir
export STOREDIR=\$TESTDIR/store
#export STOREDIR=/dev/shm/oaaziz/store
export SOCKDIR=\$TESTDIR/run/ldmsd/\${SLURM_NODEID}
export LOGDIR=\$TESTDIR/logs
export LDMSD_PIDFILE=\$TESTDIR/run/ldmsd/\${SLURM_NODEID}.pid
export LDMSD_SOCKPATH=\$TESTDIR/run
export LDMS_AUTH_FILE=\$HOME/.ldmsauth.conf
# setup file tree for ldms info
mkdir -p \$STOREDIR \$SOCKDIR \$LOGDIR \$LDMSD_SOCKPATH
# set up security for the daemon
if ! test -f \$LDMS_AUTH_FILE; then
	echo "secretword=\${RANDOM}\${RANDOM}\${RANDOM}\${RANDOM}\${RANDOM}" > \$LDMS_AUTH_FILE
fi
chmod -R go-rwx \$TESTDIR \$LDMS_AUTH_FILE
# set log to ERROR if not already set.
# other values are
if test -z "\$LDMSD_LOG_LEVEL"; then
	LDMSD_LOG_LEVEL=ERROR
fi

if test "0" = "\$SLURM_NODEID"; then
	echo "logs and data stored under \$TESTDIR"
fi
EOF

}

####################################################################
# Creates a file in job start dir for use with all jobs under ldms
# Once created, it can be adjusted manually and will not be
# overwritten.
# Multiple customized files can be managed for various jobs.
# This version puts all sets from the same plugin in the same schema,
# so you cannot have schema defined differently on different nodes.
function ldms_default_plugins {
	# edit this function carefully. better to edit the output instead
	if test -f $1; then
		echo "Using existing ldms plugin config file $1."
		return
	fi
	echo "Warning: generating default ldms plugin config file $1."
	if test -z "$LDMSD_INTERVAL_DEFAULT"; then
		LDMSD_INTERVAL_DEFAULT=1000000
	fi
	cat << EOF > $1

## /proc/meminfo memory metric
load name=meminfo
config name=meminfo producer=\${MYhostname} instance=\${SLURM_NODEID}/meminfo schema=meminfo component_id=\${SLURM_NODEID}
start name=meminfo interval=${LDMSD_INTERVAL_DEFAULT} offset=0

## /proc/stat cpu metrics
load name=procstat
config name=procstat producer=\${MYhostname} instance=\${SLURM_NODEID}/procstat schema=procstat component_id=\${SLURM_NODEID}
start name=procstat interval=${LDMSD_INTERVAL_DEFAULT} offset=0

## /proc/vmstat 
load name=vmstat
config name=vmstat producer=\${MYhostname} instance=\${SLURM_NODEID}/vmstat schema=vmstat component_id=\${SLURM_NODEID}
start name=vmstat interval=${LDMSD_INTERVAL_DEFAULT} offset=0

# can add your own stuff here if wanted.

EOF
}

####################################################################
# Creates a suffix file in job start dir for use with node 0 as aggregator.
# Multiple customized files can be managed for various jobs.
# Schema must be same on each compute node for the same sampler plugin.
function ldms_job_agg_store {
	if test -f $1; then
		echo "Warning: overwriting ldms agg/store config file $1."
	fi
	if test -z "$LDMSAGGD_CONNECTION_RETRY_INTERVAL"; then
		LDMSAGGD_CONNECTION_RETRY_INTERVAL=1000000
	fi
	if test -z "$LDMSD_INTERVAL_DEFAULT"; then
		LDMSD_INTERVAL_DEFAULT=1000000
	fi
	cat << EOF > $1
# direct each plugin to common per-plugin store file (isomorphic case)
loglevel level=INFO
loglevel level=DEBUG

#prdcr_subscribe stream=kokkos-perf-data regex=.*
#prdcr_subscribe stream=darshanConnector regex=.*
#prdcr_subscribe stream=caliper-perf-data regex=.*

#prdcr_start_regex regex=.*

load name=store_csv
config name=store_csv path=${STOREDIR} buffer=300000 buffertype=3
#config name=store_csv action=init path=${STOREDIR}
load name=stream_csv_store
#config name=stream_csv_store path=/data/OVIS/LDMS_STREAMS_CSV container=csv stream=kokkos-perf-data
##config name=stream_csv_store path=/data/OVIS/LDMS_STREAMS_CSV container=csv stream=darshanConnector
config name=stream_csv_store path=${STOREDIR} container=csv buffer=0 stream=caliper-perf-data

EOF
# add storage policy for each set on each host independently (general case).
#
# If all hosts created all schemas equally, the outer loop
# could go away. This is not the general case, however.
#
for s in $sampler_plugins; do
	sampler_schema=$s
	echo "# plugin $s samplers with schema name $sampler_schema" >> $1
	cat  << EOF >> $1
strgp_add name=$s plugin=store_csv container=store_csv schema=$sampler_schema
strgp_prdcr_add name=$s regex=.*
strgp_start name=$s
EOF
done

for i in `scontrol show "hostnames=$SLURM_NODELIST"`; do
	cat  << EOF >> $1
# add $i producer and updater loop to aggregator for
prdcr_add name=$i host=$i type=active xprt=$XPRT interval=$LDMSAGGD_CONNECTION_RETRY_INTERVAL port=$NETPORT
prdcr_start name=$i

EOF
done

cat << EOF >> $1
#prdcr_subscribe stream=kokkos-perf-data regex=.*
#prdcr_subscribe stream=darshanConnector regex=.*
prdcr_subscribe stream=caliper-perf-data regex=.*

EOF

for i in `scontrol show "hostnames=$SLURM_NODELIST"`; do
        cat  << EOF >> $1
updtr_add name=$i interval=$LDMSD_INTERVAL_DEFAULT offset=300000
updtr_prdcr_add name=$i regex=$i
updtr_start name=$i

EOF
done

}

##################################################################
# grope the load name=plugin statements out of the sampler script
# excluding comments
function find_sampler_plugins {
	if test -z "$1"; then
		return 1
	fi
	if ! test -f $1; then
		return 1
	fi
	sampler_plugins=`grep "load name=" $1 |grep -v '.*#.*load name=' |grep -v store | cut -d' ' -f 2 |sed -e 's/name=//g' -e 's/;.*//g'` 2>/dev/null
	echo $sampler_plugins
}

#################### main #########################################
#

jobnum=$1
jdir=$2
lenv=$3
dconfig=$4
export jdir

if test -z "$jobnum"; then
	echo "$0: need job id as first argument and work dir as second"
	exit 1
fi

if test -z "$jdir"; then
	echo "$0: need job ldms data directory as second argument"
	exit 1
fi

if test "0" = "$SLURM_NODEID"; then
	if test -n "ldms_startclean"; then
		echo "Warning: starting with all fresh ldmsd files".
		/bin/rm -rf $3 $4 \
			$jdir/store \
			$jdir/run \
			$jdir/logs \
			$jdir/vg.*.log \
			$jdir/.ldmsenv \
			$jdir/agg_ls.sh \
			$jdir/samp_ls.sh

	fi
	# generate default env, if not already present
	ldms_default_env $lenv
else
	sleep 2
fi


if ! test -f $lenv; then
	echo $0: unable to find/create ldms environment file $lenv
	exit 1
fi

# load ldmsd run environment variables
. $lenv
(cd $jdir; cp $lenv .ldmsenv)

echo $MYhostname $SLURM_NODEID

# This number of threads may need adjustment if running an aggregator
NUMTHREADS=4

# This number of threads may need adjustment
AGGNUMTHREADS=8

# fixed port here means only one ldmsd collector per node can run
NETPORT=60411
AGGPORT=$(( $NETPORT + 1 ))
XPRT=sock
AGGXPRT=$XPRT

allconfig=$SOCKDIR/all-config.local

# generate default config if name not given or is of missing file
if test -z "$dconfig"; then
	dconfig=$SOCKDIR/samp.config
	ldms_default_plugins $dconfig
else
	if ! test -f $dconfig; then
		ldms_default_plugins $dconfig
	fi
fi

if test "0" = "$SLURM_NODEID"; then
	aggconfig=$SOCKDIR/agg-config.gen
	sampler_plugins=$(find_sampler_plugins $dconfig)
	export sampler_plugins
	if test -z "$sampler_plugins"; then
		echo "no sampler plugins found loaded in $dconfig"
		exit 1
	fi

	ldms_job_agg_store $aggconfig

	n_sampler_plugins=$(wc -w <<< "$sampler_plugins")
	export n_sampler_plugins
fi

cat $dconfig > $allconfig

logfile=$LOGDIR/ldmsd.$MYhostname.log.$SLURM_NODEID
env > $logfile.env
echo "ldmsd starting on $MYhostname"
# to debug with valgrind, uncomment the valgrind line and 
# join with ldmsd start line.
# valgrind -v --log-file=$jdir/vg.$MYhostname.log --trace-children=yes
#-a $LDMS_AUTH_FILE \
ldmsd -c $allconfig \
	-x $XPRT:$NETPORT \
	-P $NUMTHREADS \
	-l $logfile \
	-F \
	-v $LDMSD_LOG_LEVEL &> $logfile.start &
sampler_pid=$!
echo $sampler_pid > $LDMSD_PIDFILE
# tinkers can add other options here by inserting continuation lines
# before the final redirect
echo "ldmsd started on $MYhostname"
sleep 2 ; # give collectors time to start
# start the aggregator
if test "0" = "$SLURM_NODEID"; then
	if test -z "$LDMSD_MEM_SZ"; then
		# man page suggestion is
		# LDMSD_MEM_SZ="$(($n_sampler_plugins * $SLURM_NNODES *4))k"
		# We assume array researchers are more likely with slurm
		# serious folk will set LDMSD_MEM_SZ in the launch environment
		LDMSD_MEM_SZ="$(( $n_sampler_plugins * $SLURM_NNODES ))M"
	fi
	export LDMSD_MEM_SZ
	# dump stuff for ls in dot file
	scontrol show "hostnames=$SLURM_NODELIST" > $LDMSD_SOCKPATH/nodelist
	nodelist=`cat $LDMSD_SOCKPATH/nodelist`
	echo $nodelist
	(cd $jdir; cat << EOF > agg_ls.sh
#!/bin/bash
# job was $SLURM_JOBID
export PATH=$PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH
#set -x
#ldms_ls -h $MYhostname -p $AGGPORT -x $AGGXPRT -m $LDMSD_MEM_SZ -a $LDMS_AUTH_FILE \$*
ldms_ls -h $MYhostname -p $AGGPORT -x $AGGXPRT -m $LDMSD_MEM_SZ \$*
EOF

cat << EOF > samp_ls.sh
#!/bin/bash
# job was $SLURM_JOBID
export PATH=$PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH
#set -x
EOF

for i in $nodelist; do
	cat << EOF >> samp_ls.sh
#ldms_ls -h $i -p $NETPORT -x $XPRT -a $LDMS_AUTH_FILE \$*
ldms_ls -h $i -p $NETPORT -x $XPRT \$*
EOF
done
chmod u+rx agg_ls.sh samp_ls.sh
)
	alogfile=$LOGDIR/ldmsd.$SLURM_NODEID.agg.log
	echo "ldms-aggd starting on $MYhostname"
	# to debug with valgrind, uncomment the valgrind line and 
	# join with ldmsd start line.
	# valgrind -v --log-file=$jdir/vg.agg.log --trace-children=yes
                #-a $LDMS_AUTH_FILE \
	ldms-aggd \
		-F \
		-l $alogfile \
		-c $aggconfig \
		-m $LDMSD_MEM_SZ \
		-x $AGGXPRT:$AGGPORT \
		-P $AGGNUMTHREADS \
		-v $LDMSD_LOG_LEVEL &> $alogfile.start &
	agg_pid=$!
	echo $agg_pid > $LDMSD_PIDFILE.agg
	# tinkers can add other options here by inserting continuation lines
	# before the final redirect
	echo "ldms-aggd started on $MYhostname"
fi
if test -n "$agg_pid"; then
	wait $agg_pid
	agg_exit=$?
	echo Aggregator wait found exit $agg_exit on node $SLURM_NODEID
fi
if test -n "$sampler_pid"; then
	wait $sampler_pid
	sampler_ext=$?
	echo Sampler wait found exit $agg_exit on node $SLURM_NODEID
fi
exit 0
